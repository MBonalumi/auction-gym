{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regression:       Context $\\Rightarrow$ Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import torch as th\n",
    "# from torchsummary import summary\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "###########                    MODIFY STUFF HERE                    ###############\n",
    "###################################################################################\n",
    "\n",
    "DATASETNAME = './data/10mln_data_samples_NEW.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that in this dataset, the agent was assigned an object with some context\n",
    "\n",
    "So, you cannot reuse these models on other agents with potentially different items' contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting npz data\n",
    "data = np.load(DATASETNAME, allow_pickle=True)\n",
    "files = data.files\n",
    "for f in files:\n",
    "    print(f)\n",
    "    # print(data[f].shape)\n",
    "    # # if at least two dimensions, average on first\n",
    "    # if data[f].ndim > 1:\n",
    "    #     print(data[f][0].mean(axis=0))\n",
    "    # else:\n",
    "    #     print(data[f][0].mean())\n",
    "\n",
    "contexts, values, bids, prices, outcomes, estimated_CTRs, won_masks = (data[f] for f in files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>bids</th>\n",
       "      <th>prices</th>\n",
       "      <th>outcomes</th>\n",
       "      <th>estimated_CTRs</th>\n",
       "      <th>won_mask</th>\n",
       "      <th>contexts1</th>\n",
       "      <th>contexts2</th>\n",
       "      <th>contexts3</th>\n",
       "      <th>contexts4</th>\n",
       "      <th>contexts5</th>\n",
       "      <th>contexts6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.293715</td>\n",
       "      <td>0.036494</td>\n",
       "      <td>0.036494</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>False</td>\n",
       "      <td>1.647339</td>\n",
       "      <td>0.917488</td>\n",
       "      <td>1.066935</td>\n",
       "      <td>0.047673</td>\n",
       "      <td>0.916655</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.293715</td>\n",
       "      <td>0.057794</td>\n",
       "      <td>0.057794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044673</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.152193</td>\n",
       "      <td>-1.473888</td>\n",
       "      <td>1.028854</td>\n",
       "      <td>-1.93496</td>\n",
       "      <td>-0.239937</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.293715</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>True</td>\n",
       "      <td>0.613123</td>\n",
       "      <td>-0.20033</td>\n",
       "      <td>-0.436868</td>\n",
       "      <td>0.519842</td>\n",
       "      <td>-0.476579</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.293715</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.474333</td>\n",
       "      <td>-1.944265</td>\n",
       "      <td>-1.307753</td>\n",
       "      <td>1.086831</td>\n",
       "      <td>-0.050604</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.293715</td>\n",
       "      <td>0.028522</td>\n",
       "      <td>0.028522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.282649</td>\n",
       "      <td>-0.585658</td>\n",
       "      <td>-0.472588</td>\n",
       "      <td>0.586337</td>\n",
       "      <td>-0.663535</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values      bids    prices outcomes estimated_CTRs won_mask contexts1  \\\n",
       "0  1.293715  0.036494  0.036494        0       0.028208    False  1.647339   \n",
       "1  1.293715  0.057794  0.057794        0       0.044673    False -0.152193   \n",
       "2  1.293715  0.031547  0.031547        0       0.024385     True  0.613123   \n",
       "3  1.293715  0.016949  0.016949        0       0.013101    False -0.474333   \n",
       "4  1.293715  0.028522  0.028522        0       0.022046    False -1.282649   \n",
       "\n",
       "  contexts2 contexts3 contexts4 contexts5 contexts6  \n",
       "0  0.917488  1.066935  0.047673  0.916655       1.0  \n",
       "1 -1.473888  1.028854  -1.93496 -0.239937       1.0  \n",
       "2  -0.20033 -0.436868  0.519842 -0.476579       1.0  \n",
       "3 -1.944265 -1.307753  1.086831 -0.050604       1.0  \n",
       "4 -0.585658 -0.472588  0.586337 -0.663535       1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### INSPECT DATASET ####\n",
    "\n",
    "# AVERAGE VALUE OF OUTCOMES\n",
    "# print(contexts.transpose(1,0).shape)\n",
    "contexts1, contexts2, contexts3, contexts4, contexts5, contexts6 = contexts.transpose(1,0)\n",
    "# df= pd.DataFrame.from_dict({item: data[item] for item in data.files}, orient='index')\n",
    "# df.head()\n",
    "\n",
    "d = {item: data[item] for item in data.files if data[item].ndim == 1}\n",
    "\n",
    "d['contexts1'] = contexts1\n",
    "d['contexts2'] = contexts2\n",
    "d['contexts3'] = contexts3\n",
    "d['contexts4'] = contexts4\n",
    "d['contexts5'] = contexts5\n",
    "d['contexts6'] = contexts6\n",
    "\n",
    "df = pd.DataFrame.from_dict(d, orient='index').transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38602.62506158464\n",
      "14762\n"
     ]
    }
   ],
   "source": [
    "# check data has some positive outcomes\n",
    "print(df['prices'].sum())\n",
    "print(df['outcomes'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def build_x_y(X, Y, train_test_ratio, train_validation_ratio):\n",
    "    #TEST\n",
    "    first_test_index = int(train_test_ratio * len(contexts))\n",
    "\n",
    "    x = th.from_numpy(contexts.astype(np.float32))\n",
    "    x_train = x[:first_test_index, :]\n",
    "    x_test = x[first_test_index:, :]\n",
    "\n",
    "    y = th.from_numpy(outcomes.astype(np.float32))\n",
    "    y = y.reshape(-1,1)\n",
    "    y_train = y[:first_test_index, :]\n",
    "    y_test = y[first_test_index:, :]\n",
    "\n",
    "    # VALIDATION\n",
    "    first_val_index = int(train_validation_ratio * len(x_train))\n",
    "\n",
    "    x_val = x_train[first_val_index:, :]\n",
    "    x_train = x_train[:first_val_index, :]\n",
    "    y_val = y_train[first_val_index:, :]\n",
    "    y_train = y_train[:first_val_index, :]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "\n",
    "def my_train(model, x_train, y_train, x_val, y_val, epochs, batch, device, loss, optimizer):\n",
    "\n",
    "    running_train_loss = 0.\n",
    "    last_train_loss = 0.\n",
    "    running_val_loss = 0.\n",
    "    last_val_loss = 0.\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    x_train.to(device)\n",
    "    y_train.to(device)\n",
    "\n",
    "    # for i in tqdm(range(len(x_train))):\n",
    "    for i in tqdm(range(epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        l = loss(y_pred, y_train)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += l.item()\n",
    "        l_val = loss(model(x_val), y_val)\n",
    "        running_val_loss += l_val.item()\n",
    "\n",
    "        if i % batch == 0:\n",
    "            last_train_loss = running_train_loss / batch # loss per batch\n",
    "            last_val_loss = running_val_loss / batch # loss per batch\n",
    "            print('  batch {}\\ttraining loss: {}'.format(i, last_train_loss), end=' ')\n",
    "            print('\\tvalidation loss: {}'.format(last_val_loss))\n",
    "            running_train_loss = 0.\n",
    "            running_val_loss = 0.\n",
    "\n",
    "    return last_train_loss, last_val_loss\n",
    "\n",
    "\n",
    "def my_test(model, x_test, y_test, loss):\n",
    "    cumulative_loss = 0.\n",
    "\n",
    "    for i in tqdm(range(len(x_test))):\n",
    "        a = x_test[i]\n",
    "        y_pred = model(a)\n",
    "        #loss\n",
    "        l = loss(y_pred, y_test[i])\n",
    "        cumulative_loss += l.item()\n",
    "\n",
    "    average_loss = cumulative_loss / len(x_test)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 6)\n",
      "(100000, 6)\n",
      "train:\t torch.Size([81000, 6]) torch.Size([81000, 1])\n",
      "val:\t torch.Size([9000, 6]) torch.Size([9000, 1])\n",
      "test:\t torch.Size([10000, 6]) torch.Size([10000, 1])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024900197982788086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5692c03f99fb41a4ae6c94bb3badfdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 0\ttraining loss: 0.0033955705165863038 \tvalidation loss: 0.003384522497653961\n",
      "  batch 100\ttraining loss: 0.29280158966779707 \tvalidation loss: 0.2915407080948353\n",
      "  batch 200\ttraining loss: 0.16679205060005187 \tvalidation loss: 0.16506456360220909\n",
      "  batch 300\ttraining loss: 0.08557244464755058 \tvalidation loss: 0.08514108024537563\n",
      "  batch 400\ttraining loss: 0.06821904003620148 \tvalidation loss: 0.06813966013491153\n",
      "  batch 500\ttraining loss: 0.06334397722035646 \tvalidation loss: 0.06349836077541113\n",
      "  batch 600\ttraining loss: 0.060760054551064965 \tvalidation loss: 0.06069764260202646\n",
      "  batch 700\ttraining loss: 0.05810908354818821 \tvalidation loss: 0.057931699678301814\n",
      "  batch 800\ttraining loss: 0.03461921637877822 \tvalidation loss: 0.034411395881325\n",
      "  batch 900\ttraining loss: 0.024314447287470103 \tvalidation loss: 0.02426236368715763\n",
      "  batch 1000\ttraining loss: 0.023668208718299867 \tvalidation loss: 0.023582434207201006\n",
      "  batch 1100\ttraining loss: 0.023423574548214674 \tvalidation loss: 0.02338992612436414\n",
      "  batch 1200\ttraining loss: 0.023245160169899463 \tvalidation loss: 0.023251094575971364\n",
      "  batch 1300\ttraining loss: 0.02312246263027191 \tvalidation loss: 0.02307460894808173\n",
      "  batch 1400\ttraining loss: 0.022985656261444092 \tvalidation loss: 0.02296834044158459\n",
      "  batch 1500\ttraining loss: 0.02285784512758255 \tvalidation loss: 0.0228391757234931\n",
      "  batch 1600\ttraining loss: 0.022716959975659846 \tvalidation loss: 0.02266659937798977\n",
      "  batch 1700\ttraining loss: 0.022587866969406605 \tvalidation loss: 0.022623536810278892\n",
      "  batch 1800\ttraining loss: 0.022436058204621075 \tvalidation loss: 0.022427506782114505\n",
      "  batch 1900\ttraining loss: 0.022309301011264323 \tvalidation loss: 0.02229353167116642\n",
      "  batch 2000\ttraining loss: 0.02215372685343027 \tvalidation loss: 0.022118445932865143\n",
      "  batch 2100\ttraining loss: 0.022026807367801667 \tvalidation loss: 0.02193723535165191\n",
      "  batch 2200\ttraining loss: 0.021879017148166896 \tvalidation loss: 0.021824954096227884\n",
      "  batch 2300\ttraining loss: 0.021697752308100463 \tvalidation loss: 0.021753293983638285\n",
      "  batch 2400\ttraining loss: 0.021540885772556067 \tvalidation loss: 0.021563551370054484\n",
      "  batch 2500\ttraining loss: 0.021418670304119588 \tvalidation loss: 0.021411157976835966\n",
      "  batch 2600\ttraining loss: 0.02123243661597371 \tvalidation loss: 0.021252931486815216\n",
      "  batch 2700\ttraining loss: 0.021058489959686996 \tvalidation loss: 0.021069459337741136\n",
      "  batch 2800\ttraining loss: 0.020903101358562708 \tvalidation loss: 0.02091382248327136\n",
      "  batch 2900\ttraining loss: 0.02072819920256734 \tvalidation loss: 0.020688666850328444\n",
      "  batch 3000\ttraining loss: 0.020544685050845148 \tvalidation loss: 0.020516515150666235\n",
      "  batch 3100\ttraining loss: 0.020408295821398498 \tvalidation loss: 0.020399812031537293\n",
      "  batch 3200\ttraining loss: 0.020226176623255016 \tvalidation loss: 0.020190356615930796\n",
      "  batch 3300\ttraining loss: 0.02007534498348832 \tvalidation loss: 0.020034042708575724\n",
      "  batch 3400\ttraining loss: 0.019872813113033773 \tvalidation loss: 0.019813117403537035\n",
      "  batch 3500\ttraining loss: 0.01968956757336855 \tvalidation loss: 0.01970718812197447\n",
      "  batch 3600\ttraining loss: 0.01953879853710532 \tvalidation loss: 0.01949513366445899\n",
      "  batch 3700\ttraining loss: 0.019364103283733128 \tvalidation loss: 0.01931845787912607\n",
      "  batch 3800\ttraining loss: 0.01920360902324319 \tvalidation loss: 0.019138885438442232\n",
      "  batch 3900\ttraining loss: 0.0190215895883739 \tvalidation loss: 0.019022708088159562\n",
      "  batch 4000\ttraining loss: 0.01885287255048752 \tvalidation loss: 0.018816134352236988\n",
      "  batch 4100\ttraining loss: 0.01870511397719383 \tvalidation loss: 0.018662461582571267\n",
      "  batch 4200\ttraining loss: 0.018540456909686327 \tvalidation loss: 0.018440952356904745\n",
      "  batch 4300\ttraining loss: 0.01838195888325572 \tvalidation loss: 0.018326680567115547\n",
      "  batch 4400\ttraining loss: 0.01823034130036831 \tvalidation loss: 0.018210893981158733\n",
      "  batch 4500\ttraining loss: 0.018077419735491276 \tvalidation loss: 0.01805466827005148\n",
      "  batch 4600\ttraining loss: 0.017957855835556983 \tvalidation loss: 0.017875036858022213\n",
      "  batch 4700\ttraining loss: 0.01779201492667198 \tvalidation loss: 0.017726828213781118\n",
      "  batch 4800\ttraining loss: 0.017666057012975217 \tvalidation loss: 0.017583171874284743\n",
      "  batch 4900\ttraining loss: 0.01752716939896345 \tvalidation loss: 0.017473016381263733\n",
      "  batch 5000\ttraining loss: 0.017409432865679263 \tvalidation loss: 0.01732855787500739\n",
      "  batch 5100\ttraining loss: 0.017282970678061248 \tvalidation loss: 0.017187329679727553\n",
      "  batch 5200\ttraining loss: 0.01716194847598672 \tvalidation loss: 0.017064570710062982\n",
      "  batch 5300\ttraining loss: 0.01704938553273678 \tvalidation loss: 0.016976455245167017\n",
      "  batch 5400\ttraining loss: 0.01694634325802326 \tvalidation loss: 0.016870992537587882\n",
      "  batch 5500\ttraining loss: 0.01683637807145715 \tvalidation loss: 0.016755896043032407\n",
      "  batch 5600\ttraining loss: 0.01673652797937393 \tvalidation loss: 0.016651424337178467\n",
      "  batch 5700\ttraining loss: 0.016635240837931633 \tvalidation loss: 0.01657674280926585\n",
      "  batch 5800\ttraining loss: 0.01654821828007698 \tvalidation loss: 0.016462031584233046\n",
      "  batch 5900\ttraining loss: 0.0164548753015697 \tvalidation loss: 0.01642326354980469\n",
      "  batch 6000\ttraining loss: 0.01638175219297409 \tvalidation loss: 0.01631360936909914\n",
      "  batch 6100\ttraining loss: 0.01630660956725478 \tvalidation loss: 0.016251530684530734\n",
      "  batch 6200\ttraining loss: 0.01623328445479274 \tvalidation loss: 0.01617272328585386\n",
      "  batch 6300\ttraining loss: 0.016153559125959872 \tvalidation loss: 0.01607849394902587\n",
      "  batch 6400\ttraining loss: 0.016090342402458192 \tvalidation loss: 0.01602606112137437\n",
      "  batch 6500\ttraining loss: 0.016015938129276038 \tvalidation loss: 0.015961092580109835\n",
      "  batch 6600\ttraining loss: 0.015962400641292333 \tvalidation loss: 0.015901403855532407\n",
      "  batch 6700\ttraining loss: 0.015904307588934897 \tvalidation loss: 0.01583207482472062\n",
      "  batch 6800\ttraining loss: 0.015850066374987364 \tvalidation loss: 0.01580765406601131\n",
      "  batch 6900\ttraining loss: 0.0157977245002985 \tvalidation loss: 0.015743852546438574\n",
      "  batch 7000\ttraining loss: 0.015746743641793726 \tvalidation loss: 0.015706123895943166\n",
      "  batch 7100\ttraining loss: 0.015704642329365016 \tvalidation loss: 0.015639665825292467\n",
      "  batch 7200\ttraining loss: 0.01565594389103353 \tvalidation loss: 0.015597172332927584\n",
      "  batch 7300\ttraining loss: 0.015612652990967035 \tvalidation loss: 0.01557028630748391\n",
      "  batch 7400\ttraining loss: 0.015578078450635075 \tvalidation loss: 0.015521238567307591\n",
      "  batch 7500\ttraining loss: 0.015538376439362764 \tvalidation loss: 0.015493844971060752\n",
      "  batch 7600\ttraining loss: 0.015502628982067107 \tvalidation loss: 0.015459539759904146\n",
      "  batch 7700\ttraining loss: 0.015471148565411567 \tvalidation loss: 0.015419148029759527\n",
      "  batch 7800\ttraining loss: 0.015436913445591926 \tvalidation loss: 0.015376857919618488\n",
      "  batch 7900\ttraining loss: 0.015411423351615668 \tvalidation loss: 0.015362700754776597\n",
      "  batch 8000\ttraining loss: 0.01537824645638466 \tvalidation loss: 0.015322663253173233\n",
      "  batch 8100\ttraining loss: 0.015355516830459237 \tvalidation loss: 0.01531226135790348\n",
      "  batch 8200\ttraining loss: 0.015328011522069574 \tvalidation loss: 0.015281143374741077\n",
      "  batch 8300\ttraining loss: 0.015308044226840139 \tvalidation loss: 0.01525988162495196\n",
      "  batch 8400\ttraining loss: 0.015282038152217865 \tvalidation loss: 0.015238336054608226\n",
      "  batch 8500\ttraining loss: 0.01526650325395167 \tvalidation loss: 0.015217066975310445\n",
      "  batch 8600\ttraining loss: 0.015244703236967326 \tvalidation loss: 0.015200577965006233\n",
      "  batch 8700\ttraining loss: 0.01522571051493287 \tvalidation loss: 0.015174471158534288\n",
      "  batch 8800\ttraining loss: 0.015210284236818552 \tvalidation loss: 0.015163564467802643\n",
      "  batch 8900\ttraining loss: 0.015194605868309737 \tvalidation loss: 0.015145235555246472\n",
      "  batch 9000\ttraining loss: 0.015179650029167532 \tvalidation loss: 0.015135488463565707\n",
      "  batch 9100\ttraining loss: 0.015164849683642387 \tvalidation loss: 0.015120462290942669\n",
      "  batch 9200\ttraining loss: 0.015151128508150577 \tvalidation loss: 0.015111765544861554\n",
      "  batch 9300\ttraining loss: 0.015139104658737778 \tvalidation loss: 0.015091238180175423\n",
      "  batch 9400\ttraining loss: 0.015125150959938764 \tvalidation loss: 0.0150874406658113\n",
      "  batch 9500\ttraining loss: 0.015114572783932089 \tvalidation loss: 0.015066224168986082\n",
      "  batch 9600\ttraining loss: 0.015102635491639376 \tvalidation loss: 0.015070192897692323\n",
      "  batch 9700\ttraining loss: 0.015094116535037757 \tvalidation loss: 0.015053759347647428\n",
      "  batch 9800\ttraining loss: 0.015085394605994224 \tvalidation loss: 0.015038544414564967\n",
      "  batch 9900\ttraining loss: 0.015073703918606043 \tvalidation loss: 0.01503919173963368\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016113996505737305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a61061dbe4c0c8603a419f4351873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss (error prob.) is:  0.01302087077293522\n"
     ]
    }
   ],
   "source": [
    "th.random.manual_seed(42)\n",
    "\n",
    "# model parameters\n",
    "layers = [6,16,4,1]\n",
    "dropout = 0.2\n",
    "activation = th.nn.ReLU()\n",
    "\n",
    "# dataset amount\n",
    "use_only = 100000\n",
    "\n",
    "model_layers = []\n",
    "for i in range(len(layers)-1):\n",
    "    model_layers.append(th.nn.Linear(layers[i], layers[i+1]))\n",
    "    if i < len(layers)-2:\n",
    "        model_layers.append(th.nn.Dropout(dropout))\n",
    "        model_layers.append(activation)\n",
    "    else:\n",
    "        model_layers.append(th.nn.Sigmoid())\n",
    "\n",
    "model = th.nn.Sequential(*model_layers)\n",
    "\n",
    "print(contexts.shape)\n",
    "\n",
    "contexts_old = contexts\n",
    "outcomes_old = outcomes\n",
    "contexts = contexts[ :use_only ,  :]\n",
    "outcomes = outcomes[ :use_only]\n",
    "\n",
    "print(contexts.shape)\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = build_x_y(contexts, outcomes, 0.9, 0.9)\n",
    "\n",
    "print(\"train:\\t\", x_train.shape, y_train.shape)\n",
    "print(\"val:\\t\", x_val.shape, y_val.shape)\n",
    "print(\"test:\\t\", x_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "'''=======================\n",
    "        TRAINING\n",
    "======================='''\n",
    "loss = th.nn.MSELoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10000\n",
    "device = 'cpu'\n",
    "# batch = int(epochs / 10)\n",
    "batch = 100\n",
    "\n",
    "last_train_loss, last_val_loss = my_train(model, x_train, y_train, x_val, y_val, epochs, batch, device, loss, optimizer)\n",
    "\n",
    "\n",
    "'''=======================\n",
    "        TESTING\n",
    "======================='''\n",
    "average_loss = my_test(model, x_test, y_test, loss)\n",
    "\n",
    "print(\"average loss (error prob.) is: \", average_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save the model\n",
    "ts = time.strftime(\"%Y%m%d-%H%M\", time.localtime())\n",
    "th.save(model, \"./models/nn_ctr_estimator_test/nn_ctr_\"+ts+\".pt\")\n",
    "\n",
    "filename = \"./models/nn_ctr_estimator_test/nn_ctr_\"+ts+\"_description.txt\"\n",
    "#open file\n",
    "f = open(filename, \"w\")\n",
    "#write\n",
    "f.write(\"model description:\\n\")\n",
    "f.write(str(model))\n",
    "f.write(\"\\n\\n\\n\")\n",
    "f.write(\"model parameters:\\n\")\n",
    "f.write(\"layers: \"+str(layers)+\"\\n\")\n",
    "f.write(\"dropout: \"+str(dropout)+\"\\n\")\n",
    "f.write(\"activation: \"+str(activation)+\"\\n\")\n",
    "f.write(\"\\n\\n\")\n",
    "f.write(\"training parameters:\\n\")\n",
    "f.write(\"epochs: \"+str(epochs)+\"\\n\")\n",
    "#f.write(\"batch: \"+str(batch)+\"\\n\") only used to display loss\n",
    "f.write(\"optimizer: \"+str(optimizer)+\"\\n\")\n",
    "f.write(\"loss: \"+str(loss)+\"\\n\")\n",
    "f.write(\"\\n\\n\")\n",
    "f.write(\"results:\\n\")\n",
    "f.write(\"last train loss: \"+str(last_train_loss)+\"\\n\")\n",
    "f.write(\"last val loss: \"+str(last_val_loss)+\"\\n\")\n",
    "f.write(\"average loss (error prob.) is: \"+str(average_loss)+\"\\n\")\n",
    "#close file\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00801539421081543,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c35f4fd6ef47adbaa3a9f7606b9242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 0\ttraining loss: 0.0030190441757440567 \tvalidation loss: 0.002998082898557186\n",
      "  batch 5\ttraining loss: 0.015104143507778644 \tvalidation loss: 0.015088490210473537\n",
      "  batch 10\ttraining loss: 0.01511098798364401 \tvalidation loss: 0.015060930512845516\n",
      "  batch 15\ttraining loss: 0.015102998912334442 \tvalidation loss: 0.015083630755543708\n",
      "  batch 20\ttraining loss: 0.015099073573946954 \tvalidation loss: 0.015075460076332092\n",
      "  batch 25\ttraining loss: 0.015111139602959156 \tvalidation loss: 0.015076599828898907\n",
      "  batch 30\ttraining loss: 0.01510429810732603 \tvalidation loss: 0.015065214782953262\n",
      "  batch 35\ttraining loss: 0.015100200287997722 \tvalidation loss: 0.015094640478491784\n",
      "  batch 40\ttraining loss: 0.0150959812104702 \tvalidation loss: 0.015072726272046566\n",
      "  batch 45\ttraining loss: 0.015100816637277603 \tvalidation loss: 0.015073267742991447\n",
      "  batch 50\ttraining loss: 0.015106389857828617 \tvalidation loss: 0.01507485844194889\n",
      "  batch 55\ttraining loss: 0.015101350843906403 \tvalidation loss: 0.015082699805498123\n",
      "  batch 60\ttraining loss: 0.015097056329250336 \tvalidation loss: 0.015052309073507786\n",
      "  batch 65\ttraining loss: 0.01509704701602459 \tvalidation loss: 0.01506262607872486\n",
      "  batch 70\ttraining loss: 0.015103903599083424 \tvalidation loss: 0.01507544182240963\n",
      "  batch 75\ttraining loss: 0.015099924430251122 \tvalidation loss: 0.015056415647268295\n",
      "  batch 80\ttraining loss: 0.015108274668455124 \tvalidation loss: 0.015079503133893012\n",
      "  batch 85\ttraining loss: 0.015104381740093232 \tvalidation loss: 0.015064124949276447\n",
      "  batch 90\ttraining loss: 0.015101850964128971 \tvalidation loss: 0.015065801329910755\n",
      "  batch 95\ttraining loss: 0.015108935721218585 \tvalidation loss: 0.015080604702234268\n",
      "  batch 100\ttraining loss: 0.015094087272882462 \tvalidation loss: 0.01506000068038702\n",
      "  batch 105\ttraining loss: 0.01510831993073225 \tvalidation loss: 0.015050436370074749\n",
      "  batch 110\ttraining loss: 0.015093876421451569 \tvalidation loss: 0.015068108774721622\n",
      "  batch 115\ttraining loss: 0.015105483308434486 \tvalidation loss: 0.015063693188130856\n",
      "  batch 120\ttraining loss: 0.015100154466927052 \tvalidation loss: 0.015060246735811234\n",
      "  batch 125\ttraining loss: 0.015104098618030548 \tvalidation loss: 0.015074176341295242\n",
      "  batch 130\ttraining loss: 0.015105056203901767 \tvalidation loss: 0.015059037506580353\n",
      "  batch 135\ttraining loss: 0.015103347972035408 \tvalidation loss: 0.015087929926812649\n",
      "  batch 140\ttraining loss: 0.015097416751086713 \tvalidation loss: 0.015060483291745187\n",
      "  batch 145\ttraining loss: 0.015103537030518055 \tvalidation loss: 0.01506666000932455\n",
      "  batch 150\ttraining loss: 0.015099346078932286 \tvalidation loss: 0.01507588941603899\n",
      "  batch 155\ttraining loss: 0.015108206123113633 \tvalidation loss: 0.015079527534544468\n",
      "  batch 160\ttraining loss: 0.015098294429481029 \tvalidation loss: 0.015066579543054104\n",
      "  batch 165\ttraining loss: 0.015094854310154915 \tvalidation loss: 0.015059280954301358\n",
      "  batch 170\ttraining loss: 0.015106293931603431 \tvalidation loss: 0.015066890977323055\n",
      "  batch 175\ttraining loss: 0.0151018088683486 \tvalidation loss: 0.01506765391677618\n",
      "  batch 180\ttraining loss: 0.01510524656623602 \tvalidation loss: 0.015064127556979657\n",
      "  batch 185\ttraining loss: 0.015104649774730205 \tvalidation loss: 0.015051409788429736\n",
      "  batch 190\ttraining loss: 0.01510167270898819 \tvalidation loss: 0.01507984958589077\n",
      "  batch 195\ttraining loss: 0.015100180357694625 \tvalidation loss: 0.01505664549767971\n",
      "  batch 200\ttraining loss: 0.015106505900621413 \tvalidation loss: 0.015063795447349548\n",
      "  batch 205\ttraining loss: 0.015103540755808354 \tvalidation loss: 0.01508152112364769\n",
      "  batch 210\ttraining loss: 0.015107273496687413 \tvalidation loss: 0.015080702863633633\n",
      "  batch 215\ttraining loss: 0.015102028660476208 \tvalidation loss: 0.015073990263044835\n",
      "  batch 220\ttraining loss: 0.01510324776172638 \tvalidation loss: 0.015060454979538918\n",
      "  batch 225\ttraining loss: 0.015109934099018573 \tvalidation loss: 0.015068218484520913\n",
      "  batch 230\ttraining loss: 0.015103041753172874 \tvalidation loss: 0.015072532184422015\n",
      "  batch 235\ttraining loss: 0.01510627195239067 \tvalidation loss: 0.015064495615661144\n",
      "  batch 240\ttraining loss: 0.015093768388032914 \tvalidation loss: 0.015082238242030144\n",
      "  batch 245\ttraining loss: 0.015101815573871136 \tvalidation loss: 0.015072575397789478\n",
      "  batch 250\ttraining loss: 0.015106216445565224 \tvalidation loss: 0.015078443847596646\n",
      "  batch 255\ttraining loss: 0.015106442198157311 \tvalidation loss: 0.015075843781232834\n",
      "  batch 260\ttraining loss: 0.015103761106729507 \tvalidation loss: 0.015085829794406891\n",
      "  batch 265\ttraining loss: 0.01510692946612835 \tvalidation loss: 0.015080507844686508\n",
      "  batch 270\ttraining loss: 0.015095805749297142 \tvalidation loss: 0.015080496296286583\n",
      "  batch 275\ttraining loss: 0.015103727579116821 \tvalidation loss: 0.015085635706782341\n",
      "  batch 280\ttraining loss: 0.015104557946324348 \tvalidation loss: 0.015075071714818478\n",
      "  batch 285\ttraining loss: 0.015097779594361782 \tvalidation loss: 0.015076695568859577\n",
      "  batch 290\ttraining loss: 0.015100114047527313 \tvalidation loss: 0.015029555186629295\n",
      "  batch 295\ttraining loss: 0.015102901682257652 \tvalidation loss: 0.015087241306900978\n",
      "  batch 300\ttraining loss: 0.015102080442011357 \tvalidation loss: 0.01507286336272955\n",
      "  batch 305\ttraining loss: 0.015105399303138256 \tvalidation loss: 0.015058467350900172\n",
      "  batch 310\ttraining loss: 0.015104809403419494 \tvalidation loss: 0.015086747705936432\n",
      "  batch 315\ttraining loss: 0.015100068226456643 \tvalidation loss: 0.015073234029114247\n",
      "  batch 320\ttraining loss: 0.015104812011122704 \tvalidation loss: 0.015057947300374509\n",
      "  batch 325\ttraining loss: 0.015101873688399792 \tvalidation loss: 0.01505449991673231\n",
      "  batch 330\ttraining loss: 0.01511166598647833 \tvalidation loss: 0.015110469423234463\n",
      "  batch 335\ttraining loss: 0.015105153433978558 \tvalidation loss: 0.015061936900019645\n",
      "  batch 340\ttraining loss: 0.015107473731040955 \tvalidation loss: 0.015090521611273288\n",
      "  batch 345\ttraining loss: 0.015099927224218846 \tvalidation loss: 0.015085110440850258\n",
      "  batch 350\ttraining loss: 0.015110914595425129 \tvalidation loss: 0.015077577345073223\n",
      "  batch 355\ttraining loss: 0.015106960013508797 \tvalidation loss: 0.015060062892735005\n",
      "  batch 360\ttraining loss: 0.015102745220065116 \tvalidation loss: 0.015075125545263291\n",
      "  batch 365\ttraining loss: 0.01510556973516941 \tvalidation loss: 0.015058796107769012\n",
      "  batch 370\ttraining loss: 0.015109479427337646 \tvalidation loss: 0.015049613639712333\n",
      "  batch 375\ttraining loss: 0.01510514598339796 \tvalidation loss: 0.01506284810602665\n",
      "  batch 380\ttraining loss: 0.015091934986412526 \tvalidation loss: 0.015064888447523118\n",
      "  batch 385\ttraining loss: 0.015105685591697693 \tvalidation loss: 0.015052558481693267\n",
      "  batch 390\ttraining loss: 0.015097853541374207 \tvalidation loss: 0.015084413439035415\n",
      "  batch 395\ttraining loss: 0.015098047815263272 \tvalidation loss: 0.015046916343271733\n",
      "  batch 400\ttraining loss: 0.01510663740336895 \tvalidation loss: 0.015071345120668411\n",
      "  batch 405\ttraining loss: 0.015100087225437164 \tvalidation loss: 0.015063793957233429\n",
      "  batch 410\ttraining loss: 0.015105640143156051 \tvalidation loss: 0.01505871545523405\n",
      "  batch 415\ttraining loss: 0.015107592567801476 \tvalidation loss: 0.0150665745139122\n",
      "  batch 420\ttraining loss: 0.015099197067320347 \tvalidation loss: 0.015084786899387836\n",
      "  batch 425\ttraining loss: 0.015101256780326366 \tvalidation loss: 0.015039624273777008\n",
      "  batch 430\ttraining loss: 0.01509503461420536 \tvalidation loss: 0.015053298696875573\n",
      "  batch 435\ttraining loss: 0.015103660151362419 \tvalidation loss: 0.015077728033065795\n",
      "  batch 440\ttraining loss: 0.015108975768089294 \tvalidation loss: 0.015061971731483936\n",
      "  batch 445\ttraining loss: 0.015111023373901845 \tvalidation loss: 0.015037940815091133\n",
      "  batch 450\ttraining loss: 0.015101103484630585 \tvalidation loss: 0.015061069652438164\n",
      "  batch 455\ttraining loss: 0.015105337835848331 \tvalidation loss: 0.015061677992343902\n",
      "  batch 460\ttraining loss: 0.015099299885332585 \tvalidation loss: 0.01507058497518301\n",
      "  batch 465\ttraining loss: 0.015109546296298505 \tvalidation loss: 0.015057812817394734\n",
      "  batch 470\ttraining loss: 0.015103434212505817 \tvalidation loss: 0.015071055106818675\n",
      "  batch 475\ttraining loss: 0.015109111927449704 \tvalidation loss: 0.015075019001960755\n",
      "  batch 480\ttraining loss: 0.015099113434553146 \tvalidation loss: 0.015065960772335529\n",
      "  batch 485\ttraining loss: 0.01509709283709526 \tvalidation loss: 0.01508927009999752\n",
      "  batch 490\ttraining loss: 0.015106148831546307 \tvalidation loss: 0.015045125037431717\n",
      "  batch 495\ttraining loss: 0.015100522153079509 \tvalidation loss: 0.015083459764719009\n",
      "  batch 500\ttraining loss: 0.015099766105413437 \tvalidation loss: 0.015042510069906711\n",
      "  batch 505\ttraining loss: 0.015094290673732757 \tvalidation loss: 0.015072033368051052\n",
      "  batch 510\ttraining loss: 0.015107336081564426 \tvalidation loss: 0.015075355023145675\n",
      "  batch 515\ttraining loss: 0.015104497037827968 \tvalidation loss: 0.015057225525379182\n",
      "  batch 520\ttraining loss: 0.015105947852134705 \tvalidation loss: 0.015051965788006782\n",
      "  batch 525\ttraining loss: 0.0151059003546834 \tvalidation loss: 0.015041914582252503\n",
      "  batch 530\ttraining loss: 0.015097630769014358 \tvalidation loss: 0.015078585781157016\n",
      "  batch 535\ttraining loss: 0.015107577480375767 \tvalidation loss: 0.015042210184037685\n",
      "  batch 540\ttraining loss: 0.015103951655328273 \tvalidation loss: 0.015071872994303704\n",
      "  batch 545\ttraining loss: 0.015110986866056919 \tvalidation loss: 0.015092373080551624\n",
      "  batch 550\ttraining loss: 0.015095516480505467 \tvalidation loss: 0.015088879317045212\n",
      "  batch 555\ttraining loss: 0.015097279287874698 \tvalidation loss: 0.015072347782552242\n",
      "  batch 560\ttraining loss: 0.015111985430121423 \tvalidation loss: 0.015065955370664597\n",
      "  batch 565\ttraining loss: 0.01510053388774395 \tvalidation loss: 0.015092820674180985\n",
      "  batch 570\ttraining loss: 0.015100710652768612 \tvalidation loss: 0.015070735104382038\n",
      "  batch 575\ttraining loss: 0.01509634144604206 \tvalidation loss: 0.015053152665495872\n",
      "  batch 580\ttraining loss: 0.015111671388149261 \tvalidation loss: 0.015070529282093048\n",
      "  batch 585\ttraining loss: 0.015109262615442275 \tvalidation loss: 0.015084586665034294\n",
      "  batch 590\ttraining loss: 0.015108681470155715 \tvalidation loss: 0.015095816738903523\n",
      "  batch 595\ttraining loss: 0.015101596526801587 \tvalidation loss: 0.015082868933677673\n",
      "  batch 600\ttraining loss: 0.015107416547834873 \tvalidation loss: 0.015063055977225303\n",
      "  batch 605\ttraining loss: 0.015102955512702465 \tvalidation loss: 0.015078447759151459\n",
      "  batch 610\ttraining loss: 0.015099023096263408 \tvalidation loss: 0.015073265507817268\n",
      "  batch 615\ttraining loss: 0.01510414369404316 \tvalidation loss: 0.015084224380552768\n",
      "  batch 620\ttraining loss: 0.015093795955181122 \tvalidation loss: 0.015050180815160274\n",
      "  batch 625\ttraining loss: 0.015108754672110081 \tvalidation loss: 0.015062396414577961\n",
      "  batch 630\ttraining loss: 0.015094489231705666 \tvalidation loss: 0.015087506920099258\n",
      "  batch 635\ttraining loss: 0.015103103965520859 \tvalidation loss: 0.015055747330188751\n",
      "  batch 640\ttraining loss: 0.015100859478116036 \tvalidation loss: 0.015067926980555058\n",
      "  batch 645\ttraining loss: 0.015107519179582595 \tvalidation loss: 0.015064907446503639\n",
      "  batch 650\ttraining loss: 0.015103053115308285 \tvalidation loss: 0.015125098638236523\n",
      "  batch 655\ttraining loss: 0.015099150873720647 \tvalidation loss: 0.015074980445206165\n",
      "  batch 660\ttraining loss: 0.015103830955922604 \tvalidation loss: 0.015048098005354404\n",
      "  batch 665\ttraining loss: 0.015102505497634412 \tvalidation loss: 0.015079972520470619\n",
      "  batch 670\ttraining loss: 0.015108498558402062 \tvalidation loss: 0.01506911739706993\n",
      "  batch 675\ttraining loss: 0.015096101723611356 \tvalidation loss: 0.01506966594606638\n",
      "  batch 680\ttraining loss: 0.015098049119114876 \tvalidation loss: 0.015097163990139962\n",
      "  batch 685\ttraining loss: 0.015111719816923141 \tvalidation loss: 0.015086593851447106\n",
      "  batch 690\ttraining loss: 0.01510511226952076 \tvalidation loss: 0.015064679645001888\n",
      "  batch 695\ttraining loss: 0.015098896622657777 \tvalidation loss: 0.015062252804636956\n",
      "  batch 700\ttraining loss: 0.015107884258031844 \tvalidation loss: 0.015053238905966281\n",
      "  batch 705\ttraining loss: 0.015102487988770008 \tvalidation loss: 0.01506959330290556\n",
      "  batch 710\ttraining loss: 0.015109804272651673 \tvalidation loss: 0.015062706172466278\n",
      "  batch 715\ttraining loss: 0.015104550682008267 \tvalidation loss: 0.015076754428446294\n",
      "  batch 720\ttraining loss: 0.015113286860287189 \tvalidation loss: 0.015081593953073024\n",
      "  batch 725\ttraining loss: 0.015108630247414111 \tvalidation loss: 0.015062767080962658\n",
      "  batch 730\ttraining loss: 0.01509954985231161 \tvalidation loss: 0.015038884617388249\n",
      "  batch 735\ttraining loss: 0.015103279612958431 \tvalidation loss: 0.015059249848127365\n",
      "  batch 740\ttraining loss: 0.015111836791038512 \tvalidation loss: 0.015070763416588306\n",
      "  batch 745\ttraining loss: 0.015107730403542519 \tvalidation loss: 0.015083328448235988\n",
      "  batch 750\ttraining loss: 0.015103304386138916 \tvalidation loss: 0.01508485022932291\n",
      "  batch 755\ttraining loss: 0.015098553150892258 \tvalidation loss: 0.015073994360864162\n",
      "  batch 760\ttraining loss: 0.015105302259325982 \tvalidation loss: 0.01506447996944189\n",
      "  batch 765\ttraining loss: 0.015093667060136795 \tvalidation loss: 0.01506903227418661\n",
      "  batch 770\ttraining loss: 0.015100641176104546 \tvalidation loss: 0.01508240755647421\n",
      "  batch 775\ttraining loss: 0.015105312131345272 \tvalidation loss: 0.01506747268140316\n",
      "  batch 780\ttraining loss: 0.015102771483361721 \tvalidation loss: 0.015062114410102367\n",
      "  batch 785\ttraining loss: 0.015101260133087635 \tvalidation loss: 0.015053250081837177\n",
      "  batch 790\ttraining loss: 0.015108832903206349 \tvalidation loss: 0.015060623921453953\n",
      "  batch 795\ttraining loss: 0.015107464790344239 \tvalidation loss: 0.015087630972266198\n",
      "  batch 800\ttraining loss: 0.015096582472324371 \tvalidation loss: 0.015088715590536594\n",
      "  batch 805\ttraining loss: 0.015106988325715064 \tvalidation loss: 0.01509178914129734\n",
      "  batch 810\ttraining loss: 0.015106856077909469 \tvalidation loss: 0.01506409514695406\n",
      "  batch 815\ttraining loss: 0.01510319598019123 \tvalidation loss: 0.01505550518631935\n",
      "  batch 820\ttraining loss: 0.015110919810831546 \tvalidation loss: 0.015086447261273861\n",
      "  batch 825\ttraining loss: 0.015105808153748512 \tvalidation loss: 0.015044236183166504\n",
      "  batch 830\ttraining loss: 0.015106693282723427 \tvalidation loss: 0.015047187730669976\n",
      "  batch 835\ttraining loss: 0.015097978711128234 \tvalidation loss: 0.015083695389330387\n",
      "  batch 840\ttraining loss: 0.01510312519967556 \tvalidation loss: 0.015074729546904564\n",
      "  batch 845\ttraining loss: 0.015104521997272968 \tvalidation loss: 0.015081925690174103\n",
      "  batch 850\ttraining loss: 0.015107151307165622 \tvalidation loss: 0.015054277889430524\n",
      "  batch 855\ttraining loss: 0.015106405876576901 \tvalidation loss: 0.015072136744856834\n",
      "  batch 860\ttraining loss: 0.015093900263309479 \tvalidation loss: 0.015077352896332741\n",
      "  batch 865\ttraining loss: 0.015106121078133583 \tvalidation loss: 0.015073800832033158\n",
      "  batch 870\ttraining loss: 0.015101332031190396 \tvalidation loss: 0.015098760835826397\n",
      "  batch 875\ttraining loss: 0.015104888007044792 \tvalidation loss: 0.015053901821374893\n",
      "  batch 880\ttraining loss: 0.015097268112003803 \tvalidation loss: 0.015047485195100307\n",
      "  batch 885\ttraining loss: 0.01510622762143612 \tvalidation loss: 0.015074461884796619\n",
      "  batch 890\ttraining loss: 0.01510809063911438 \tvalidation loss: 0.015055038221180438\n",
      "  batch 895\ttraining loss: 0.015093153901398182 \tvalidation loss: 0.015031409636139869\n",
      "  batch 900\ttraining loss: 0.015100868977606296 \tvalidation loss: 0.015074213966727257\n",
      "  batch 905\ttraining loss: 0.01510274801403284 \tvalidation loss: 0.015087519027292728\n",
      "  batch 910\ttraining loss: 0.01510460563004017 \tvalidation loss: 0.015068592317402364\n",
      "  batch 915\ttraining loss: 0.01509841699153185 \tvalidation loss: 0.015062542445957661\n",
      "  batch 920\ttraining loss: 0.015104676783084869 \tvalidation loss: 0.015105967409908772\n",
      "  batch 925\ttraining loss: 0.0151066729798913 \tvalidation loss: 0.015070838294923306\n",
      "  batch 930\ttraining loss: 0.015102145820856094 \tvalidation loss: 0.015060766600072384\n",
      "  batch 935\ttraining loss: 0.0150963069871068 \tvalidation loss: 0.015068457275629044\n",
      "  batch 940\ttraining loss: 0.015103554166853429 \tvalidation loss: 0.015049494989216327\n",
      "  batch 945\ttraining loss: 0.015092907473444939 \tvalidation loss: 0.015079602040350438\n",
      "  batch 950\ttraining loss: 0.015102904103696347 \tvalidation loss: 0.015057026594877242\n",
      "  batch 955\ttraining loss: 0.015099636651575566 \tvalidation loss: 0.015078004449605942\n",
      "  batch 960\ttraining loss: 0.015095673315227032 \tvalidation loss: 0.015075910277664662\n",
      "  batch 965\ttraining loss: 0.01510216947644949 \tvalidation loss: 0.015075059607625008\n",
      "  batch 970\ttraining loss: 0.015100892260670662 \tvalidation loss: 0.015045227482914925\n",
      "  batch 975\ttraining loss: 0.01510200072079897 \tvalidation loss: 0.015069994702935218\n",
      "  batch 980\ttraining loss: 0.015101975947618484 \tvalidation loss: 0.015040704235434531\n",
      "  batch 985\ttraining loss: 0.015097323618829251 \tvalidation loss: 0.01505925077944994\n",
      "  batch 990\ttraining loss: 0.015096445009112358 \tvalidation loss: 0.015063007362186909\n",
      "  batch 995\ttraining loss: 0.015112361311912537 \tvalidation loss: 0.015067448653280736\n",
      "  batch 1000\ttraining loss: 0.015112304873764516 \tvalidation loss: 0.015079030953347682\n",
      "  batch 1005\ttraining loss: 0.015102176740765572 \tvalidation loss: 0.015092908963561058\n",
      "  batch 1010\ttraining loss: 0.01510375402867794 \tvalidation loss: 0.015065289661288261\n",
      "  batch 1015\ttraining loss: 0.015110986493527889 \tvalidation loss: 0.015058670938014985\n",
      "  batch 1020\ttraining loss: 0.015104313753545285 \tvalidation loss: 0.015070782974362373\n",
      "  batch 1025\ttraining loss: 0.015108295530080796 \tvalidation loss: 0.015095863491296768\n",
      "  batch 1030\ttraining loss: 0.015096525102853775 \tvalidation loss: 0.015048486553132534\n",
      "  batch 1035\ttraining loss: 0.015111242048442363 \tvalidation loss: 0.01509682908654213\n",
      "  batch 1040\ttraining loss: 0.015101948566734791 \tvalidation loss: 0.01508201528340578\n",
      "  batch 1045\ttraining loss: 0.015101437643170356 \tvalidation loss: 0.015058467537164688\n",
      "  batch 1050\ttraining loss: 0.01510646566748619 \tvalidation loss: 0.015083780325949193\n",
      "  batch 1055\ttraining loss: 0.015103146806359291 \tvalidation loss: 0.015053143166005612\n",
      "  batch 1060\ttraining loss: 0.015106953121721745 \tvalidation loss: 0.015068316645920277\n",
      "  batch 1065\ttraining loss: 0.015099005587399007 \tvalidation loss: 0.015070400200784206\n",
      "  batch 1070\ttraining loss: 0.01510845497250557 \tvalidation loss: 0.015065990574657916\n",
      "  batch 1075\ttraining loss: 0.01510725487023592 \tvalidation loss: 0.015067428722977639\n",
      "  batch 1080\ttraining loss: 0.015106369368731975 \tvalidation loss: 0.015073824115097523\n",
      "  batch 1085\ttraining loss: 0.015107581205666066 \tvalidation loss: 0.015071912296116352\n",
      "  batch 1090\ttraining loss: 0.015103013068437577 \tvalidation loss: 0.01507953107357025\n",
      "  batch 1095\ttraining loss: 0.015104887261986732 \tvalidation loss: 0.01508814562112093\n",
      "  batch 1100\ttraining loss: 0.015102767944335937 \tvalidation loss: 0.015054310485720635\n",
      "  batch 1105\ttraining loss: 0.015101367421448231 \tvalidation loss: 0.015066369622945785\n",
      "  batch 1110\ttraining loss: 0.015106357634067535 \tvalidation loss: 0.015101037174463271\n",
      "  batch 1115\ttraining loss: 0.015104172565042973 \tvalidation loss: 0.015067953802645206\n",
      "  batch 1120\ttraining loss: 0.015107597410678863 \tvalidation loss: 0.01506862286478281\n",
      "  batch 1125\ttraining loss: 0.015102597884833813 \tvalidation loss: 0.015080971643328667\n",
      "  batch 1130\ttraining loss: 0.015112628228962421 \tvalidation loss: 0.015043767169117928\n",
      "  batch 1135\ttraining loss: 0.015103228017687797 \tvalidation loss: 0.015074300020933152\n",
      "  batch 1140\ttraining loss: 0.015106086805462837 \tvalidation loss: 0.015073306858539581\n",
      "  batch 1145\ttraining loss: 0.015106676332652568 \tvalidation loss: 0.015065461955964565\n",
      "  batch 1150\ttraining loss: 0.01509479209780693 \tvalidation loss: 0.015047595649957658\n",
      "  batch 1155\ttraining loss: 0.015105132386088372 \tvalidation loss: 0.015077449753880501\n",
      "  batch 1160\ttraining loss: 0.01510351113975048 \tvalidation loss: 0.015071913041174412\n",
      "  batch 1165\ttraining loss: 0.015106123685836793 \tvalidation loss: 0.015073103830218315\n",
      "  batch 1170\ttraining loss: 0.015102679282426834 \tvalidation loss: 0.015099218115210533\n",
      "  batch 1175\ttraining loss: 0.015108632668852807 \tvalidation loss: 0.01505485139787197\n",
      "  batch 1180\ttraining loss: 0.015102203004062176 \tvalidation loss: 0.01504624132066965\n",
      "  batch 1185\ttraining loss: 0.015106625109910964 \tvalidation loss: 0.015058454684913158\n",
      "  batch 1190\ttraining loss: 0.015111398696899415 \tvalidation loss: 0.015081685595214367\n",
      "  batch 1195\ttraining loss: 0.01510024443268776 \tvalidation loss: 0.015067476220428944\n",
      "  batch 1200\ttraining loss: 0.015096796490252019 \tvalidation loss: 0.01503204070031643\n",
      "  batch 1205\ttraining loss: 0.015110623650252819 \tvalidation loss: 0.015068834275007248\n",
      "  batch 1210\ttraining loss: 0.015101371891796589 \tvalidation loss: 0.015063463523983956\n",
      "  batch 1215\ttraining loss: 0.015096497721970081 \tvalidation loss: 0.01508112195879221\n",
      "  batch 1220\ttraining loss: 0.01509490069001913 \tvalidation loss: 0.015087220445275306\n",
      "  batch 1225\ttraining loss: 0.015100361220538617 \tvalidation loss: 0.01508006639778614\n",
      "  batch 1230\ttraining loss: 0.015096626617014408 \tvalidation loss: 0.015047962591052056\n",
      "  batch 1235\ttraining loss: 0.015103860571980477 \tvalidation loss: 0.015078816376626492\n",
      "  batch 1240\ttraining loss: 0.015103779174387455 \tvalidation loss: 0.015059643238782883\n",
      "  batch 1245\ttraining loss: 0.015104258432984352 \tvalidation loss: 0.015063965134322644\n",
      "  batch 1250\ttraining loss: 0.015112786926329136 \tvalidation loss: 0.015081454440951348\n",
      "  batch 1255\ttraining loss: 0.01510548945516348 \tvalidation loss: 0.015064599364995957\n",
      "  batch 1260\ttraining loss: 0.01509899813681841 \tvalidation loss: 0.015066123940050602\n",
      "  batch 1265\ttraining loss: 0.015101665258407592 \tvalidation loss: 0.015065793506801128\n",
      "  batch 1270\ttraining loss: 0.015107066743075848 \tvalidation loss: 0.015075216442346573\n",
      "  batch 1275\ttraining loss: 0.015101734176278114 \tvalidation loss: 0.01506745107471943\n",
      "  batch 1280\ttraining loss: 0.015106354281306267 \tvalidation loss: 0.015050334297120571\n",
      "  batch 1285\ttraining loss: 0.01510236356407404 \tvalidation loss: 0.015059920400381089\n",
      "  batch 1290\ttraining loss: 0.015100307762622833 \tvalidation loss: 0.015057678148150445\n",
      "  batch 1295\ttraining loss: 0.01510846298187971 \tvalidation loss: 0.015065321326255798\n",
      "  batch 1300\ttraining loss: 0.015110968612134457 \tvalidation loss: 0.015059762634336948\n",
      "  batch 1305\ttraining loss: 0.015095200389623642 \tvalidation loss: 0.015063756331801414\n",
      "  batch 1310\ttraining loss: 0.015101413615047932 \tvalidation loss: 0.015056301094591617\n",
      "  batch 1315\ttraining loss: 0.015102229826152325 \tvalidation loss: 0.015067034214735032\n",
      "  batch 1320\ttraining loss: 0.015110725723206996 \tvalidation loss: 0.015061892196536064\n",
      "  batch 1325\ttraining loss: 0.01510231252759695 \tvalidation loss: 0.015067156590521335\n",
      "  batch 1330\ttraining loss: 0.015105022862553597 \tvalidation loss: 0.015081525780260563\n",
      "  batch 1335\ttraining loss: 0.015106866881251336 \tvalidation loss: 0.015070913545787334\n",
      "  batch 1340\ttraining loss: 0.015100527182221412 \tvalidation loss: 0.015107943117618561\n",
      "  batch 1345\ttraining loss: 0.015107572451233864 \tvalidation loss: 0.015076641365885735\n",
      "  batch 1350\ttraining loss: 0.015104272961616516 \tvalidation loss: 0.015069201402366162\n",
      "  batch 1355\ttraining loss: 0.015102971717715264 \tvalidation loss: 0.015055897645652294\n",
      "  batch 1360\ttraining loss: 0.01510765478014946 \tvalidation loss: 0.015063520707190036\n",
      "  batch 1365\ttraining loss: 0.015114327147603034 \tvalidation loss: 0.015060176514089108\n",
      "  batch 1370\ttraining loss: 0.015102993324398994 \tvalidation loss: 0.015041729994118213\n",
      "  batch 1375\ttraining loss: 0.015101982094347478 \tvalidation loss: 0.015098091214895248\n",
      "  batch 1380\ttraining loss: 0.01511252038180828 \tvalidation loss: 0.015063837170600891\n",
      "  batch 1385\ttraining loss: 0.015095367096364498 \tvalidation loss: 0.015053369849920274\n",
      "  batch 1390\ttraining loss: 0.015104197897017002 \tvalidation loss: 0.015069231577217578\n",
      "  batch 1395\ttraining loss: 0.015100752376019955 \tvalidation loss: 0.015056322515010833\n",
      "  batch 1400\ttraining loss: 0.01510421521961689 \tvalidation loss: 0.015078806318342686\n",
      "  batch 1405\ttraining loss: 0.015102358907461167 \tvalidation loss: 0.015074524283409118\n",
      "  batch 1410\ttraining loss: 0.015107770636677741 \tvalidation loss: 0.015056958049535751\n",
      "  batch 1415\ttraining loss: 0.0151023518294096 \tvalidation loss: 0.015069631114602088\n",
      "  batch 1420\ttraining loss: 0.015102260932326316 \tvalidation loss: 0.015064057148993015\n",
      "  batch 1425\ttraining loss: 0.015105921402573585 \tvalidation loss: 0.01509267147630453\n",
      "  batch 1430\ttraining loss: 0.01510017216205597 \tvalidation loss: 0.015062952414155006\n",
      "  batch 1435\ttraining loss: 0.015107622183859349 \tvalidation loss: 0.015079761855304241\n",
      "  batch 1440\ttraining loss: 0.015104137547314166 \tvalidation loss: 0.015059161931276321\n",
      "  batch 1445\ttraining loss: 0.015105404891073704 \tvalidation loss: 0.015057930164039135\n",
      "  batch 1450\ttraining loss: 0.015107396617531776 \tvalidation loss: 0.015067687630653382\n",
      "  batch 1455\ttraining loss: 0.015101212076842785 \tvalidation loss: 0.01506427563726902\n",
      "  batch 1460\ttraining loss: 0.015109737403690815 \tvalidation loss: 0.015071721002459525\n",
      "  batch 1465\ttraining loss: 0.015107950754463672 \tvalidation loss: 0.015080550871789455\n",
      "  batch 1470\ttraining loss: 0.015092881955206394 \tvalidation loss: 0.015058682858943939\n",
      "  batch 1475\ttraining loss: 0.015104282274842262 \tvalidation loss: 0.01508264895528555\n",
      "  batch 1480\ttraining loss: 0.015106549859046936 \tvalidation loss: 0.015056838095188142\n",
      "  batch 1485\ttraining loss: 0.015102187544107437 \tvalidation loss: 0.01507174652069807\n",
      "  batch 1490\ttraining loss: 0.015099725127220154 \tvalidation loss: 0.015038843080401421\n",
      "  batch 1495\ttraining loss: 0.015102586895227432 \tvalidation loss: 0.015064284950494767\n",
      "  batch 1500\ttraining loss: 0.015105379931628703 \tvalidation loss: 0.015098769962787629\n",
      "  batch 1505\ttraining loss: 0.015110014379024506 \tvalidation loss: 0.015098755434155463\n",
      "  batch 1510\ttraining loss: 0.015102501586079597 \tvalidation loss: 0.015045377798378468\n",
      "  batch 1515\ttraining loss: 0.015105374157428741 \tvalidation loss: 0.015096724405884743\n",
      "  batch 1520\ttraining loss: 0.015096522122621536 \tvalidation loss: 0.01506668571382761\n",
      "  batch 1525\ttraining loss: 0.015103712864220142 \tvalidation loss: 0.015066369995474815\n",
      "  batch 1530\ttraining loss: 0.015109858848154545 \tvalidation loss: 0.015075814351439476\n",
      "  batch 1535\ttraining loss: 0.015107903443276883 \tvalidation loss: 0.015062438882887363\n",
      "  batch 1540\ttraining loss: 0.01510472372174263 \tvalidation loss: 0.015061366744339466\n",
      "  batch 1545\ttraining loss: 0.015107642486691475 \tvalidation loss: 0.01506199836730957\n",
      "  batch 1550\ttraining loss: 0.015107451006770134 \tvalidation loss: 0.015048732049763203\n",
      "  batch 1555\ttraining loss: 0.015101723186671735 \tvalidation loss: 0.015078667178750038\n",
      "  batch 1560\ttraining loss: 0.015101449564099312 \tvalidation loss: 0.015078508853912353\n",
      "  batch 1565\ttraining loss: 0.015103645436465741 \tvalidation loss: 0.015088224038481712\n",
      "  batch 1570\ttraining loss: 0.015107396245002746 \tvalidation loss: 0.015092072635889053\n",
      "  batch 1575\ttraining loss: 0.015109468437731265 \tvalidation loss: 0.015058194659650326\n",
      "  batch 1580\ttraining loss: 0.015099775046110153 \tvalidation loss: 0.015071236155927181\n",
      "  batch 1585\ttraining loss: 0.015099630691111088 \tvalidation loss: 0.015086220763623714\n",
      "  batch 1590\ttraining loss: 0.015107126720249654 \tvalidation loss: 0.01508406102657318\n",
      "  batch 1595\ttraining loss: 0.015098781697452069 \tvalidation loss: 0.015073912404477596\n",
      "  batch 1600\ttraining loss: 0.015101490542292595 \tvalidation loss: 0.0150817995890975\n",
      "  batch 1605\ttraining loss: 0.015098579414188861 \tvalidation loss: 0.015053673088550568\n",
      "  batch 1610\ttraining loss: 0.015108086168766022 \tvalidation loss: 0.015053291991353034\n",
      "  batch 1615\ttraining loss: 0.015099399164319039 \tvalidation loss: 0.015054815635085105\n",
      "  batch 1620\ttraining loss: 0.015110616199672222 \tvalidation loss: 0.01507491823285818\n",
      "  batch 1625\ttraining loss: 0.015106125548481942 \tvalidation loss: 0.015091467089951039\n",
      "  batch 1630\ttraining loss: 0.015102142468094826 \tvalidation loss: 0.01509330254048109\n",
      "  batch 1635\ttraining loss: 0.015113694965839386 \tvalidation loss: 0.015072454139590264\n",
      "  batch 1640\ttraining loss: 0.015098709240555763 \tvalidation loss: 0.015103555656969548\n",
      "  batch 1645\ttraining loss: 0.015101261250674725 \tvalidation loss: 0.015057206340134143\n",
      "  batch 1650\ttraining loss: 0.015106940641999245 \tvalidation loss: 0.015037130750715732\n",
      "  batch 1655\ttraining loss: 0.015104782208800316 \tvalidation loss: 0.015056797303259373\n",
      "  batch 1660\ttraining loss: 0.015100523829460144 \tvalidation loss: 0.015052266418933868\n",
      "  batch 1665\ttraining loss: 0.015109851211309432 \tvalidation loss: 0.015077744796872138\n",
      "  batch 1670\ttraining loss: 0.015100305899977684 \tvalidation loss: 0.015065282769501209\n",
      "  batch 1675\ttraining loss: 0.015107190236449242 \tvalidation loss: 0.01508235428482294\n",
      "  batch 1680\ttraining loss: 0.015108630061149597 \tvalidation loss: 0.015059438720345496\n",
      "  batch 1685\ttraining loss: 0.015110797621309758 \tvalidation loss: 0.015052541345357894\n",
      "  batch 1690\ttraining loss: 0.015103041380643844 \tvalidation loss: 0.015074835903942586\n",
      "  batch 1695\ttraining loss: 0.015110019780695438 \tvalidation loss: 0.015059883333742618\n",
      "  batch 1700\ttraining loss: 0.015103395842015744 \tvalidation loss: 0.01506965272128582\n",
      "  batch 1705\ttraining loss: 0.015108219161629678 \tvalidation loss: 0.015069796703755856\n",
      "  batch 1710\ttraining loss: 0.015108639933168888 \tvalidation loss: 0.015082606673240661\n",
      "  batch 1715\ttraining loss: 0.015104274824261665 \tvalidation loss: 0.015071703866124154\n",
      "  batch 1720\ttraining loss: 0.015106518939137458 \tvalidation loss: 0.015070752240717411\n",
      "  batch 1725\ttraining loss: 0.015106582269072532 \tvalidation loss: 0.01507263518869877\n",
      "  batch 1730\ttraining loss: 0.01510334126651287 \tvalidation loss: 0.015088182128965854\n",
      "  batch 1735\ttraining loss: 0.015104054473340511 \tvalidation loss: 0.015089697390794753\n",
      "  batch 1740\ttraining loss: 0.015102403424680232 \tvalidation loss: 0.015062122233211994\n",
      "  batch 1745\ttraining loss: 0.015096371620893478 \tvalidation loss: 0.015067172795534134\n",
      "  batch 1750\ttraining loss: 0.015112579055130481 \tvalidation loss: 0.015074160136282443\n",
      "  batch 1755\ttraining loss: 0.015106454864144326 \tvalidation loss: 0.0150652963668108\n",
      "  batch 1760\ttraining loss: 0.015109135396778583 \tvalidation loss: 0.01507878489792347\n",
      "  batch 1765\ttraining loss: 0.015109662897884846 \tvalidation loss: 0.015061985142529011\n",
      "  batch 1770\ttraining loss: 0.015098871104419232 \tvalidation loss: 0.015062714181840419\n",
      "  batch 1775\ttraining loss: 0.01509828045964241 \tvalidation loss: 0.015059291012585163\n",
      "  batch 1780\ttraining loss: 0.015102256275713443 \tvalidation loss: 0.015091193467378616\n",
      "  batch 1785\ttraining loss: 0.015108854323625565 \tvalidation loss: 0.015063580870628358\n",
      "  batch 1790\ttraining loss: 0.015104868076741695 \tvalidation loss: 0.015052725747227668\n",
      "  batch 1795\ttraining loss: 0.015099362470209599 \tvalidation loss: 0.015067006461322308\n",
      "  batch 1800\ttraining loss: 0.015091056376695633 \tvalidation loss: 0.01507354062050581\n",
      "  batch 1805\ttraining loss: 0.015107507444918155 \tvalidation loss: 0.015042934380471707\n",
      "  batch 1810\ttraining loss: 0.015106511861085891 \tvalidation loss: 0.015064670890569686\n",
      "  batch 1815\ttraining loss: 0.01510024182498455 \tvalidation loss: 0.01504532303661108\n",
      "  batch 1820\ttraining loss: 0.015098826214671134 \tvalidation loss: 0.015047646127641201\n",
      "  batch 1825\ttraining loss: 0.015113114751875401 \tvalidation loss: 0.015053924359381198\n",
      "  batch 1830\ttraining loss: 0.015109171532094479 \tvalidation loss: 0.015059542283415795\n",
      "  batch 1835\ttraining loss: 0.015101569332182407 \tvalidation loss: 0.015059730596840381\n",
      "  batch 1840\ttraining loss: 0.015104213543236256 \tvalidation loss: 0.015055743418633938\n",
      "  batch 1845\ttraining loss: 0.015118720009922981 \tvalidation loss: 0.015094571560621262\n",
      "  batch 1850\ttraining loss: 0.015108008310198784 \tvalidation loss: 0.015077066607773304\n",
      "  batch 1855\ttraining loss: 0.015105602890253067 \tvalidation loss: 0.01505834236741066\n",
      "  batch 1860\ttraining loss: 0.015100882202386857 \tvalidation loss: 0.01508344803005457\n",
      "  batch 1865\ttraining loss: 0.015105346962809563 \tvalidation loss: 0.01507194209843874\n",
      "  batch 1870\ttraining loss: 0.015100319497287273 \tvalidation loss: 0.01507989652454853\n",
      "  batch 1875\ttraining loss: 0.015109662711620332 \tvalidation loss: 0.015086680836975574\n",
      "  batch 1880\ttraining loss: 0.015102451480925084 \tvalidation loss: 0.015077869221568108\n",
      "  batch 1885\ttraining loss: 0.015104682743549347 \tvalidation loss: 0.015077135898172856\n",
      "  batch 1890\ttraining loss: 0.015095947496592998 \tvalidation loss: 0.015067258104681968\n",
      "  batch 1895\ttraining loss: 0.015108644962310791 \tvalidation loss: 0.015056974068284035\n",
      "  batch 1900\ttraining loss: 0.015095643512904645 \tvalidation loss: 0.015067292563617229\n",
      "  batch 1905\ttraining loss: 0.015101638063788414 \tvalidation loss: 0.01507677249610424\n",
      "  batch 1910\ttraining loss: 0.015106264688074588 \tvalidation loss: 0.01508837565779686\n",
      "  batch 1915\ttraining loss: 0.015106757916510106 \tvalidation loss: 0.01505808886140585\n",
      "  batch 1920\ttraining loss: 0.015105120092630386 \tvalidation loss: 0.015089659579098224\n",
      "  batch 1925\ttraining loss: 0.015108700841665268 \tvalidation loss: 0.015056170523166656\n",
      "  batch 1930\ttraining loss: 0.01510023269802332 \tvalidation loss: 0.015050223097205162\n",
      "  batch 1935\ttraining loss: 0.01510831918567419 \tvalidation loss: 0.015066881850361823\n",
      "  batch 1940\ttraining loss: 0.01510975807905197 \tvalidation loss: 0.015080270171165467\n",
      "  batch 1945\ttraining loss: 0.015108933858573436 \tvalidation loss: 0.01507434956729412\n",
      "  batch 1950\ttraining loss: 0.015113369189202785 \tvalidation loss: 0.015076353773474693\n",
      "  batch 1955\ttraining loss: 0.015101996250450611 \tvalidation loss: 0.015064868703484535\n",
      "  batch 1960\ttraining loss: 0.015102497674524784 \tvalidation loss: 0.015076029486954213\n",
      "  batch 1965\ttraining loss: 0.015110667422413826 \tvalidation loss: 0.015080680884420872\n",
      "  batch 1970\ttraining loss: 0.015101020224392414 \tvalidation loss: 0.015097002126276493\n",
      "  batch 1975\ttraining loss: 0.015103667974472046 \tvalidation loss: 0.015080985240638256\n",
      "  batch 1980\ttraining loss: 0.015096466802060605 \tvalidation loss: 0.015066849812865258\n",
      "  batch 1985\ttraining loss: 0.015101861394941806 \tvalidation loss: 0.01509449202567339\n",
      "  batch 1990\ttraining loss: 0.015103587508201599 \tvalidation loss: 0.015049441531300545\n",
      "  batch 1995\ttraining loss: 0.01511202473193407 \tvalidation loss: 0.015091621503233909\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016125917434692383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4bcf3270174c5fa2253d79eca1627b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss (error prob.) is:  0.013035350477946522\n"
     ]
    }
   ],
   "source": [
    "# MODEL NAME\n",
    "MODELNAME = \"./models/nn_ctr_estimator_test/nn_ctr_20230623-1641.pt\"\n",
    "\n",
    "loaded_model = th.load(MODELNAME)\n",
    "loaded_model.eval()\n",
    "\n",
    "already_trained = 100000\n",
    "new_data_amt = 100000\n",
    "\n",
    "new_x = contexts[already_trained : already_trained + new_data_amt, :]\n",
    "new_y = outcomes[already_trained : already_trained + new_data_amt]\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = build_x_y(new_x, new_y, 0.9, 0.9)\n",
    "\n",
    "loss = th.nn.MSELoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 2000\n",
    "device = 'cpu'\n",
    "# batch = int(epochs / 10)\n",
    "batch = 100\n",
    "\n",
    "last_train_loss, last_val_loss = my_train(loaded_model, x_train, y_train, x_val, y_val, epochs, batch, device, loss, optimizer)\n",
    "\n",
    "average_loss = my_test(loaded_model, x_test, y_test, loss)\n",
    "\n",
    "print(\"average loss (error prob.) is: \", average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelsMine import BIGPR\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
